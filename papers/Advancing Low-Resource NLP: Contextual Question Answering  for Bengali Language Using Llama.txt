Advancing Low-Resource NLP: Contextual Question Answering  for Bengali Language Using Llama

Abstract—Natural language processing (NLP) has witnessed significant advancements in recent years,  particularly in improving question-answering (QA) sys- tems for well-resourced languages such as English.  However, the development of such systems for low- resource languages, including Bengali, remains insuf- ficiently explored. This study proposes an approach to  developing a Bengali QA system utilizing the Llama- 3.2-3B-Instruct model, leveraging transfer learning  techniques on a synthetic dataset derived from the SQuAD 2.0 benchmark. The experiments achieved an F1 score of 42.77%, marking a 4.02% improvement over the previous best performance of multilingual BERT (mBERT) variants. These results establish a benchmark against human responses and underscore  the potential of transfer learning in advancing QA capa- bilities for Bengali and similar low-resource languages.

Keywords—Natural Language Processing, Question Answering, Large Language Models, Llama Model, Fine-Tuning, Bengali Dataset.

Conclusion:
This paper introduced a Bengali question answering system using the Llama-3.2-3B-Instruct model, leveraging transfer learning and LoRA techniques to address the  low-resource nature of the Bengali language. By fine- tuning the model on a synthetic dataset derived from  SQuAD 2.0, we achieved an F1 score of 42.77% and an Exact Match (EM) score of 14.22%, establishing a foundational benchmark for Bengali QA systems. The results demonstrate the effectiveness of large language  models like Llama in improving QA performance for low- resource languages. Future directions include improving  dataset quality, exploring alternative model architectures, and incorporating advanced techniques such as RLHF (Reinforcement Learning with Human Feedback). This  approach lays a framework for developing NLP systems for underrepresented languages and can be adapted to other low-resource languages beyond Bengali.
